{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f818530-d1dc-4ebf-bf9a-f6f536879bf8",
   "metadata": {},
   "source": [
    "Copyright 2024 Google, LLC. This software is provided as-is,\n",
    "without warranty or representation for any use or purpose. Your\n",
    "use of it is subject to your agreement with Google.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "# Example Agent Workflow using Google's ADK\n",
    "\n",
    "This notebook provides an example of building an agentic workflow with Google's new ADK. For more information please visit https://google.github.io/adk-docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Nest Support System with Vertex AI RAG and ADK\n",
    "\n",
    "This notebook will help setup the RAG Engine and BQ Datasets used in this ADK Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-deps-markdown",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "Install the necessary libraries for Vertex AI, Google Cloud, ADK, and HTTP requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-libs-markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libs-code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vertex AI Modules\n",
    "import vertexai\n",
    "from vertexai.preview import rag\n",
    "\n",
    "# Vertex AI Platform Modules\n",
    "from google.cloud import aiplatform_v1beta1 as aiplatform # This module helps parse the info for delete_rag_corpa function\n",
    "\n",
    "# Google Cloud Storage\n",
    "from google.cloud import storage\n",
    "\n",
    "#Google BigQuery\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "# Other Python Modules\n",
    "import os\n",
    "from typing import List, Dict, TypedDict, Any\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b9076-6af8-402b-860b-c417d4e5780b",
   "metadata": {},
   "source": [
    "Ignore warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8623395-c614-427b-b637-04b7f5cf915b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Important:** Update the `project_id`, `corpa_document_bucket`, `local_documents`, and `ticket_server_url` variables below with your specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-code",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = \"YOUR_PROJECT_ID\" # Your GCP Project ID\n",
    "location = \"global\" # You can leave this setting as global\n",
    "region = \"us-central1\" # Your region. This notebook has only been tested in us-central1\n",
    "\n",
    "corpa_name = \"nest-rag-corpus\" # This will be the display name of your RAG Engine corpus\n",
    "bq_dataset_id = f\"{project_id}.Product_Inventory\"\n",
    "bq_table_id = f\"{bq_dataset_id}.product_data\"\n",
    "\n",
    "corpa_document_bucket = \"gs://YOUR_BUCKET_ID/nest/docs/\" # The GCS path to the files you want to ingest into your RAG Engine corpus\n",
    "bq_data_bucket = \"gs://YOUR_BUCKET_ID/wip/bq_import/\" # The GCS path to the files you want to ingest into your BQ datastore\n",
    "\n",
    "support_documents = \"./nest_docs/\" # Local directory containing Nest support files to copy\n",
    "bq_data = \"./bq_data/\" # Local directory containing BQ data files\n",
    "\n",
    "ticket_server_url = \"http://localhost:8001\" # The url to the mock ticket system. This will be a GCE VM running the ticket_server.py web service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-vertex-markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Setup and Vertex AI Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc59be-1eab-4125-8cd0-d85792f49619",
   "metadata": {},
   "source": [
    "Set environment variables for Google libraries and initiate the vertex ai client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-vertex-code",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"1\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a2315-45c2-4a87-a0b8-de428adbd2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertexai.init(project=project_id, location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcs-setup-markdown",
   "metadata": {},
   "source": [
    "## 4. Google Cloud Storage Setup\n",
    "\n",
    "This function checks if the specified GCS bucket and folder exist, creates them if necessary, and uploads documents from the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e88b26-f815-49f2-aed9-a4a1c7e37734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bucket(bucket_path: str):\n",
    "    parsed_uri = urlparse(bucket_path)\n",
    "    bucket_name = parsed_uri.netloc\n",
    "    prefix = parsed_uri.path.lstrip('/')\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket object\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Check if the bucket exists, create it if not\n",
    "    if not bucket.exists():\n",
    "        bucket.create()\n",
    "        print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "\n",
    "    # Create the folder prefix if it doesn't implicitly exist\n",
    "    if prefix:\n",
    "        blob_name = f\"{prefix}\" if prefix.endswith('/') else f\"{prefix}/\"\n",
    "        placeholder_blob = bucket.blob(blob_name + \".placeholder\")\n",
    "        if not placeholder_blob.exists():\n",
    "            placeholder_blob.upload_from_string('')\n",
    "            print(f\"Simulated folder '{bucket_path}' created.\")\n",
    "        else:\n",
    "            print(f\"Simulated folder '{bucket_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b9b02-92ae-4f62-b3c5-d81b1b2e782b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_files(folder_path: str, bucket_path: str):\n",
    "    parsed_uri = urlparse(bucket_path)\n",
    "    bucket_name = parsed_uri.netloc\n",
    "    prefix = parsed_uri.path.lstrip('/')\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket object\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "        for filename in os.listdir(folder_path):\n",
    "            local_file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(local_file_path):\n",
    "                gcs_blob_name = f\"{prefix}{filename}\"\n",
    "                blob = bucket.blob(gcs_blob_name)\n",
    "                blob.upload_from_filename(local_file_path)\n",
    "                print(f\"Uploaded '{local_file_path}' to 'gs://{bucket_name}/{gcs_blob_name}'\")\n",
    "    else:\n",
    "        print(f\"Local directory '{folder_path}' does not exist or is not a directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8868578-8de1-426d-8ed0-5bd228c7b964",
   "metadata": {},
   "source": [
    "Create the GCS buckets for the support documents and BQ data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b509aa-b954-47c4-b8e5-6f5cf9415924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_bucket(corpa_document_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589aab4-b9c0-4d90-846a-1f2c69b78507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_bucket(bq_data_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d285ce3-d291-45e6-94f6-dba02915ac72",
   "metadata": {},
   "source": [
    "Upload the Nest support documents and BQ data files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d76a29-3453-423a-aee7-7d0142275960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "upload_files(support_documents, corpa_document_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a162c-dab1-467e-aed2-1036902404ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "upload_files(bq_data, bq_data_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-funcs-markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions\n",
    "\n",
    "Define functions for interacting with the agent, managing RAG corpora, and defining tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-manage-funcs-code",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Define RAG Corpus Management Functions\n",
    "\n",
    "# NOTE: The delete function is defined but not used in the main flow.\n",
    "# It can be useful for cleanup. It expects a pager object, which is\n",
    "# typically obtained from `rag.list_corpora()`.\n",
    "def delete_rag_corpora(rag_corpora_pager: aiplatform.services.vertex_rag_data_service.pagers.ListRagCorporaPager):\n",
    "    \"\"\"\n",
    "    Deletes all RAG corpora listed in the provided pager object.\n",
    "    USE WITH CAUTION! THIS WILL PERMANENTLY DELETE CORPORA.\n",
    "\n",
    "    Args:\n",
    "        rag_corpora_pager: The pager object from rag.list_corpora().\n",
    "    \"\"\"\n",
    "    names_list = []\n",
    "    print(\"Identifying corpora to delete...\")\n",
    "    try:\n",
    "        for rag_corpus_obj in rag_corpora_pager: # Iterate through the actual RagCorpus objects\n",
    "            if hasattr(rag_corpus_obj, 'name'):\n",
    "                print(f\" - Found corpus: {rag_corpus_obj.display_name} ({rag_corpus_obj.name})\")\n",
    "                names_list.append(rag_corpus_obj.name)\n",
    "            else:\n",
    "                print(f\" - Skipping object without a 'name' attribute: {rag_corpus_obj}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error iterating through corpora pager: {e}\")\n",
    "        return # Stop if we can't list them properly\n",
    "\n",
    "    if not names_list:\n",
    "        print(\"No corpora found to delete.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nStarting deletion process...\")\n",
    "    deleted_count = 0\n",
    "    failed_count = 0\n",
    "    for corpus_name_to_delete in names_list:\n",
    "        print(f\"Attempting to delete corpus: {corpus_name_to_delete}...\")\n",
    "        try:\n",
    "            # Optional: You might want to double-check existence before deleting\n",
    "            # rag.get_corpus(name=corpus_name_to_delete)\n",
    "            rag.delete_corpus(name=corpus_name_to_delete, force=True) # Use force=True to delete non-empty corpora\n",
    "            print(f\"  Successfully deleted {corpus_name_to_delete}\")\n",
    "            deleted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to delete {corpus_name_to_delete}: {e}\")\n",
    "            failed_count += 1\n",
    "    print(f\"\\nDeletion complete. Deleted: {deleted_count}, Failed: {failed_count}\")\n",
    "\n",
    "\n",
    "def create_rag_corpora(display_name, source_bucket):\n",
    "    EMBEDDING_MODEL = \"publishers/google/models/text-embedding-004\"  # @param {type:\"string\", isTemplate: true}\n",
    "    embedding_model_config = rag.EmbeddingModelConfig(publisher_model=EMBEDDING_MODEL)\n",
    "\n",
    "    rag_corpus = rag.create_corpus(\n",
    "        display_name=display_name, embedding_model_config=embedding_model_config\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    INPUT_GCS_BUCKET = (\n",
    "        source_bucket\n",
    "    )\n",
    "\n",
    "    response = rag.import_files(\n",
    "        corpus_name=rag_corpus.name,\n",
    "        paths=[INPUT_GCS_BUCKET],\n",
    "        chunk_size=1024,  # Optional\n",
    "        chunk_overlap=100,  # Optional\n",
    "        max_embedding_requests_per_min=900,  # Optional\n",
    "    )\n",
    "    \n",
    "    # This code shows how to upload local files to the corpus. \n",
    "    #rag_file = rag.upload_file(\n",
    "    #    corpus_name=rag_corpus.name,\n",
    "    #    path=\"./test.txt\",\n",
    "    #    display_name=\"test.txt\",\n",
    "    #    description=\"my test file\"\n",
    "    #)\n",
    "    \n",
    "    return rag_corpus\n",
    "\n",
    "    \n",
    "\n",
    "print(\"RAG corpus management functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976d07d-9589-4eac-aad1-210205d65f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gcs_uri(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves Google Cloud Storage (GCS) URIs for documents relevant to a given query.\n",
    "\n",
    "    This function queries a pre-configured Retrieval-Augmented Generation (RAG)\n",
    "    corpus to find documents related to the input query string. It extracts\n",
    "    the source GCS URIs from the top relevant documents identified by the\n",
    "    RAG system based on semantic similarity. Use this function when you need\n",
    "    to find the source files in GCS that contain information related to a\n",
    "    specific question or topic.\n",
    "\n",
    "    Args:\n",
    "        query: str - The natural language query or topic to search for within\n",
    "                 the RAG corpus. For example: \"What were the Q3 sales figures?\"\n",
    "                 or \"Tell me about project Alpha's latest status\".\n",
    "\n",
    "    Returns:\n",
    "         str - A JSON string representing a list of unique GCS URIs. These URIs\n",
    "               point to the source documents found to be relevant to the query.\n",
    "               Returns a JSON string representing an empty list ('[]') if no\n",
    "               relevant documents meet the similarity criteria.\n",
    "               Example return value: '[\"gs://my-bucket/doc1.pdf\", \"gs://my-bucket/report_q3.txt\"]'\n",
    "    \"\"\"\n",
    "    query_response = rag.retrieval_query(\n",
    "        rag_resources=[\n",
    "            rag.RagResource(\n",
    "                rag_corpus=rag_corpus.name,\n",
    "                # Optional: supply IDs from `rag.list_files()`.\n",
    "                # rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\n",
    "            )\n",
    "        ],\n",
    "        text=f'''\n",
    "        {query}\n",
    "        ''',\n",
    "        similarity_top_k=10,  # Optional\n",
    "        vector_distance_threshold=0.5,  # Optional\n",
    "    )\n",
    "    #print(response)\n",
    "    uri_set = set()\n",
    "    for context in query_response.contexts.contexts:\n",
    "        uri_set.add(context.source_uri)\n",
    "        #json.dumps(list(uri_set))\n",
    "    #doc_uri = uri_set.pop()\n",
    "    doc_uri = json.dumps(list(uri_set))\n",
    "    return doc_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-setup-markdown",
   "metadata": {},
   "source": [
    "## 6. RAG Corpus Setup\n",
    "\n",
    "Check if the RAG Corpus configured in step 2 exists. If not, create it and initiate file import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-setup-code",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "existing_corpora = rag.list_corpora()\n",
    "\n",
    "print(existing_corpora)\n",
    "\n",
    "# Variable to hold the corpus if found\n",
    "found_corpus = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7c1db-1fa8-4247-8e61-28c96efa9060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate through all existing RAG corpora\n",
    "for corpus in existing_corpora.rag_corpora: # Ensure you iterate the correct attribute\n",
    "    # Check if display_name exists and matches\n",
    "    if getattr(corpus, 'display_name', None) == corpa_name:\n",
    "        print(f\"Existing Corpa found. Using {corpus.name}\")\n",
    "        \n",
    "        # You already have the corpus object, no need to call get_corpus usually\n",
    "        # If 'corpus' object from the list is sufficient, use it directly.\n",
    "        # If you MUST get a fresh object or different type, uncomment the next line:\n",
    "        # rag_corpus = rag.get_corpus(name=corpus.name) \n",
    "        found_corpus = corpus # Store the found corpus object\n",
    "        \n",
    "        print(f\"This corpus contains the following files:\")\n",
    "        try:\n",
    "            # List files associated with the found corpus\n",
    "            for file in rag.list_files(corpus.name): # Use corpus.name\n",
    "                print(getattr(file, 'display_name', 'N/A')) # Safer access\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not list files for {corpus.name}. Error: {e}\")\n",
    "            \n",
    "        break # Exit the loop as soon as we find the match\n",
    "\n",
    "# After the loop, check if we found anything\n",
    "if found_corpus is None:\n",
    "    # The loop completed without finding the corpus\n",
    "    print(f\"No existing {corpa_name} resource found. Creating one now.\")\n",
    "    try:\n",
    "        rag_corpus = create_rag_corpora(corpa_name, corpa_document_bucket)\n",
    "        print(f\"New RAG corpus created at {rag_corpus.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating corpus {corpa_name}: {e}\")\n",
    "        rag_corpus = None # Indicate failure\n",
    "else:\n",
    "    # The corpus was found in the loop\n",
    "    rag_corpus = found_corpus # Assign the found corpus to the main variable\n",
    "\n",
    "# Now 'rag_corpus' holds either the found or newly created corpus (or None if creation failed)\n",
    "# You can proceed to use 'rag_corpus' here\n",
    "if rag_corpus:\n",
    "    print(f\"\\nProceeding with corpus: {rag_corpus.name}\")\n",
    "    # ... your next steps using rag_corpus ...\n",
    "else:\n",
    "    print(f\"\\nFailed to find or create corpus '{corpa_name}'. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab536faa-9525-48ed-b4f4-5be0ebded05c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = get_gcs_uri('How do I install a Nest E thermostat')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "define-subagents-markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Create BQ Dataset\n",
    "\n",
    "Create the BQ Dataset and product_data table for the BQ Agent example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a2773-b686-4d67-9746-f6c380e2f8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8aed3-98ef-4ca4-9c29-b625a4ff7adf",
   "metadata": {},
   "source": [
    "Create the BQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ca64c-d949-4bee-b209-814a0bb28330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = bigquery.Dataset(bq_dataset_id)\n",
    "\n",
    "dataset.location = \"US\"\n",
    "dataset = client.create_dataset(dataset, timeout=30)\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86053988-05c6-4f27-aad3-0da6f34ff20e",
   "metadata": {},
   "source": [
    "Create the BQ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8157a3-1079-420a-a525-7765a0d9e02f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig(source_format=bigquery.SourceFormat.AVRO)\n",
    "uri = f\"{bq_data_bucket}product_data.avro\"\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, bq_table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(bq_table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup (Optional)\n",
    "\n",
    "If you want to delete the RAG Corpus and BQ Dataset created during this session, uncomment and run the following cell. **Warning:** This permanently deletes the corpus and its indexed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-code",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Delete the RAG Corpus (USE WITH CAUTION!)\n",
    "\n",
    "# Set this flag to True only if you are sure you want to delete the corpus\n",
    "confirm_delete = False\n",
    "\n",
    "if confirm_delete == True:\n",
    "    print(f\"Attempting to delete RAG Corpus: {rag_corpus.name} ({corpa_name})\")\n",
    "    try:\n",
    "        rag.delete_corpus(name=rag_corpus.name)\n",
    "        print(f\"Successfully deleted corpus: {rag_corpus.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete corpus {rag_corpus.name}: {e}\")\n",
    "    \n",
    "    print(f\"Attempting to delete BQ Table: {bq_table_id}\")\n",
    "    try:\n",
    "        client.delete_table(bq_table_id, not_found_ok=True)\n",
    "        print(\"Deleted table '{}'.\".format(bq_table_id))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete BQ table {bq_table_id}: {e}\")\n",
    "\n",
    "    print(f\"Attempting to delete BQ Dataset: {bq_dataset_id}\")\n",
    "    try:\n",
    "        client.delete_dataset(bq_dataset_id, delete_contents=True, not_found_ok=True)\n",
    "        print(\"Deleted dataset '{}'.\".format(bq_dataset_id))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete BQ dataset {bq_dataset_id}: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping deletion: confirm_delete is set to False.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d75f2",
   "metadata": {},
   "source": [
    "**WARNING**: Uncomment this next line to delete ALL existing RAG Engine corpora within this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da36b74",
   "metadata": {},
   "source": [
    "ONLY USE THIS OPTION IN A NON PRODUCTION ENVIRONMENT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_rag_corpora(rag.list_corpora()) # This option will delete ALL RAG Engine instances."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-agent-framework-agent-framework",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
